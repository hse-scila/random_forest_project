{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.arff import loadarff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def read_and_split_data(path, test_share, target_name):\n",
    "    #function for reading and splitting data into train and test datasets\n",
    "    data = loadarff(path)\n",
    "    df = pd.DataFrame(data[0])\n",
    "    df[target_name] = LabelEncoder().fit_transform(df[target_name])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pd.get_dummies(df.drop(columns=target_name)), df[target_name], test_size=test_share, random_state=42)\n",
    "    feature_types = np.where(X_train.dtypes.values == 'float64', 'real', 'ohe')\n",
    "    return X_train, X_test, y_train, y_test, feature_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def find_best_split(x, y, alpha, beta, f_type='real'):\n",
    "    #function for finding the best split\n",
    "    x = pd.Series(x)\n",
    "    y = pd.Series(y)\n",
    "    n = y.shape[0]\n",
    "    if f_type =='real':\n",
    "        x = x.sort_values() #sorting the feature vector \n",
    "        y = y[x.index] #sorting the target vector accordingly \n",
    "        thr_inds = np.where(x.values[:-1] != x.values[1:])[0] #finding indexes where the feature changes its value\n",
    "        thresholds = (x.values[thr_inds]+x.values[thr_inds+1])/2 #finding thresholds\n",
    "        y = pd.get_dummies(y)\n",
    "        y_temp = y.cumsum()\n",
    "        y_temp = y_temp.div(y_temp.sum(axis=1), axis=0) #finding probabilities for all partitions \n",
    "        \n",
    "        if alpha == 1: # in this case, Shannon entropy-based information gain is used\n",
    "            y_temp = y_temp.values[thr_inds,:]\n",
    "            y_temp_log = np.log2(y_temp)\n",
    "            shannon_vals_1 = -np.sum(np.multiply(y_temp,y_temp_log), axis =1) #calculating Shannon entropy\n",
    "        else:\n",
    "            y_temp = y_temp.values[thr_inds,:]**alpha \n",
    "            if beta == 1: #in this case, Renyi entropy-based information gain is used\n",
    "                renyi_vals_1 = (1/(1-alpha)) * np.log2(np.sum(y_temp, axis=1)) #calculating Renyi entropy\n",
    "            else: #in this case, Sharma-Mittal entropy-based information gain is used\n",
    "                sharma_vals_1 = (1/(1-beta))*(np.power(np.sum(y_temp, axis=1), ((1-beta)/(1-alpha)))-1) #calculating Sharma-Mittal entropy\n",
    "        #doing the same for the right subtrees\n",
    "        y_temp = y[::-1].cumsum()\n",
    "        y_temp = y_temp.div(y_temp.sum(axis=1), axis=0) \n",
    "        \n",
    "        if alpha == 1:\n",
    "            y_temp = y_temp.values[n-thr_inds-2,:]\n",
    "            y_temp_log = np.log2(y_temp)\n",
    "            shannon_vals_2 = -np.sum(np.multiply(y_temp,y_temp_log), axis =1) \n",
    "            info_gain = shannon_vals_1*(thr_inds+1)/n + shannon_vals_2*(n-thr_inds-1)/n #calculating Shannon entropy-based information gain\n",
    "        else:\n",
    "            y_temp = y_temp.values[n-thr_inds-2,:]**alpha\n",
    "            if beta == 1:\n",
    "                renyi_vals_2 = (1/(1-alpha)) * np.log2(np.sum(y_temp, axis=1))\n",
    "                info_gain = renyi_vals_1*(thr_inds+1)/n + renyi_vals_2*(n-thr_inds-1)/n #calculating Renyi entropy-based information gain\n",
    "            else:\n",
    "                sharma_vals_2 = (1/(1-beta))*(np.power(np.sum(y_temp, axis=1), ((1-beta)/(1-alpha)))-1)\n",
    "                info_gain = sharma_vals_1*(thr_inds+1)/n + sharma_vals_2*(n-thr_inds-1)/n #calculating Sharma-Mittal based information gain\n",
    "            \n",
    "        try:\n",
    "            best_ind = np.argmin(info_gain) \n",
    "            best_thr = thresholds[best_ind]\n",
    "        except:\n",
    "            return 0, -1\n",
    "    else: #doing the same but for  ONE hot encoded feature, where only one splitting is possible\n",
    "        best_thr = 0.5\n",
    "        best_ind=0\n",
    "        y1 = y[x<0.5]\n",
    "        y2 = y[x>0.5]\n",
    "        n1 = y1.shape[0]\n",
    "        n2 = n-n1\n",
    "        \n",
    "        if alpha == 1:\n",
    "            y1 = y1.value_counts(normalize=True).values\n",
    "            y1_log = np.log2(y1)\n",
    "            y2 = y2.value_counts(normalize=True).values\n",
    "            y2_log = np.log2(y2)\n",
    "            info_gain = [-np.sum(np.multiply(y1,y1_log))*n1/n - np.sum(np.multiply(y2,y2_log))*n2/n]\n",
    "        else:\n",
    "            y1 = (y1.value_counts(normalize=True).values)**alpha\n",
    "            y2 = (y2.value_counts(normalize=True).values)**alpha\n",
    "            if beta == 1:\n",
    "                info_gain = [(1/(1-alpha)) * np.log2(np.sum(y1))*n1/n + (1/(1-alpha)) * np.log2(np.sum(y2))*n2/n] #calculating Renyi entropy-based information gain\n",
    "            else:\n",
    "                info_gain = [(1/(1-beta))*(np.power(np.sum(y1), ((1-beta)/(1-alpha)))-1)*(n1/n)+ (1/(1-beta))*(np.power(np.sum(y2), ((1-beta)/(1-alpha)))-1)*(n2/n)] #calculating Sharma-Mittal entropy-based information gain \n",
    "        \n",
    "    return best_thr, info_gain[best_ind] \n",
    "\n",
    "class DecisionTreeRenyi:\n",
    "    #class describing a decision tree\n",
    "    def __init__(self, feature_types, max_features, alpha, beta, max_depth=16, min_samples_split=100, min_samples_leaf=10):\n",
    "\n",
    "        self._tree = {}\n",
    "        self.feature_types = feature_types\n",
    "        self.max_features = max_features\n",
    "        self._max_depth = max_depth\n",
    "        self._min_samples_split = min_samples_split\n",
    "        self._min_samples_leaf = min_samples_leaf\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def _fit_node(self, sub_X, sub_y, node, depth=0):\n",
    "        if self._max_depth is not None and depth==self._max_depth: #max depth\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            node[\"class\"] = Counter(sub_y).most_common(1)[0][0]\n",
    "            return\n",
    "            \n",
    "        if np.all(sub_y == sub_y[0]):\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            node[\"class\"] = sub_y[0]\n",
    "            return\n",
    "        \n",
    "        if self._min_samples_split is not None and (len(sub_y)<self._min_samples_split): #min_split\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            node[\"class\"] = Counter(sub_y).most_common(1)[0][0]\n",
    "            return\n",
    "\n",
    "        feature_best, threshold_best, infogain_best, split = None, None, None, None\n",
    "        \n",
    "        for feature in np.random.choice(self.shape, self.max_features):\n",
    "            feature_vector = sub_X[:, feature]\n",
    "            threshold, infogain = find_best_split(feature_vector, sub_y, self.alpha, self.beta, f_type = self.feature_types[feature])\n",
    "             \n",
    "            if infogain_best is None or infogain > infogain_best:\n",
    "                feature_best = feature\n",
    "                infogain_best = infogain\n",
    "                split = feature_vector < threshold\n",
    "                threshold_best = threshold        \n",
    "\n",
    "        if feature_best is None or infogain_best == -1 or np.all(split) or np.all(~split):\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            node[\"class\"] = Counter(sub_y).most_common(1)[0][0]\n",
    "            return\n",
    "        \n",
    "        if self._min_samples_leaf is not None and (np.sum(split)<self._min_samples_leaf or  np.sum(~split)<self._min_samples_leaf):\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            node[\"class\"] = Counter(sub_y).most_common(1)[0][0]\n",
    "            return\n",
    "        \n",
    "        node[\"type\"] = \"nonterminal\"\n",
    "        node[\"feature_split\"] = feature_best\n",
    "        node[\"threshold\"] = threshold_best\n",
    "        node[\"left_child\"], node[\"right_child\"] = {}, {}\n",
    "        \n",
    "    \n",
    "        self._fit_node(sub_X[split], sub_y[split], node[\"left_child\"], depth=depth+1)  \n",
    "        self._fit_node(sub_X[np.logical_not(split)], sub_y[np.logical_not(split)], node[\"right_child\"], depth=depth+1)\n",
    "\n",
    "    def _predict_node(self, x, node):\n",
    "        tree = node\n",
    "        class_i = None\n",
    "        while True:\n",
    "            if tree['type'] == 'terminal':\n",
    "                class_i = tree['class']\n",
    "                break\n",
    "            else:\n",
    "                feature_split_i = tree['feature_split']\n",
    "                real_split_i = tree['threshold']\n",
    "                if x[feature_split_i] < real_split_i:\n",
    "                    tree = tree['left_child']\n",
    "                else:\n",
    "                    tree = tree['right_child']\n",
    "        return class_i\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.shape = X.shape[1]\n",
    "        self._fit_node(X, y, self._tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predicted = []\n",
    "        for x in X:\n",
    "            predicted.append(self._predict_node(x, self._tree))\n",
    "        return np.array(predicted)\n",
    "\n",
    "class Renyi_SharmaMittal_RandomForest:\n",
    "    #class describing a random forest\n",
    "    def __init__(self, n, feature_types, max_features=4, alpha=0.99, beta=1,\n",
    "                 max_depth=16, min_samples_split=20, min_samples_leaf=10):\n",
    "        \n",
    "        self.feature_types = feature_types\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.n = n\n",
    "        self.forest = []\n",
    "     \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.forest = Parallel(n_jobs=-1)(delayed(self.fit_tree)(X_train, y_train) for _ in range(self.n))\n",
    "    \n",
    "    def fit_tree(self, X_train, y_train):\n",
    "        temp = DecisionTreeRenyi(self.feature_types, self.max_features, self.alpha, self.beta, self.max_depth, \n",
    "                                     self.min_samples_split, self.min_samples_leaf)\n",
    "        temp.fit(X_train, y_train)\n",
    "        return temp\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        preds = self.forest[0].predict(X_test)\n",
    "        for i in self.forest[1:]:\n",
    "            preds = np.vstack([preds,i.predict(X_test)])\n",
    "        preds = np.swapaxes(preds,0,1)\n",
    "        preds = np.array([Counter(row).most_common(1)[0][0] for row in preds])\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/99 [00:00<?, ?it/s]\n",
      "  0%|                                                                                           | 0/99 [00:00<?, ?it/s]\u001b[AC:\\Users\\verai\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "\n",
      "  1%|▊                                                                             | 1/99 [17:46<29:01:33, 1066.26s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8e44b565acc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m99\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRenyi_SharmaMittal_RandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;31m#calculating accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-b926e2ea77e1>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, y_train)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#example for har dataset with Sharma-Mittal entropy-based information gain\n",
    "X_train, X_test, y_train, y_test, feature_types = read_and_split_data('har.arff', 0.25, 'Class')\n",
    "header = ['alpha','beta', 'acc', 'f1']\n",
    "with open('har_results_SharmaMittal_LineByLine.csv', 'a', encoding='UTF8', newline='') as myfile:\n",
    "    writer = csv.writer(myfile)\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for alpha in tqdm(np.arange(0.01, 1.0, 0.01)):\n",
    "        for beta in tqdm(np.arange(0.01, 1.0, 0.01), total=99):\n",
    "            model = Renyi_SharmaMittal_RandomForest(300, feature_types, max_features = int(X_train.shape[1]/3), alpha=alpha, beta=beta)\n",
    "            model.fit(X_train.values, y_train.values)\n",
    "            preds = model.predict(X_test.values)\n",
    "            #calculating accuracy\n",
    "            acc = sum(preds == y_test.values)/len(y_test)\n",
    "            #calculating f1 score\n",
    "            f = f1_score(y_test.values, preds, average=None)\n",
    "\n",
    "            datax = [alpha, beta, acc, f]\n",
    "            # write the data\n",
    "            writer.writerow(datax)\n",
    "            myfile.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example for har dataset with Renyi entropy-based information gain\n",
    "X_train, X_test, y_train, y_test, feature_types = read_and_split_data('har.arff', 0.25, 'Class')\n",
    "header = ['alpha','beta', 'acc', 'f1']\n",
    "beta = 1\n",
    "with open('har_results_Renyi_LineByLine.csv', 'a', encoding='UTF8', newline='') as myfile:\n",
    "    writer = csv.writer(myfile)\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for alpha in tqdm(np.arange(0.01, 1.0, 0.01)):\n",
    "        model = Renyi_SharmaMittal_RandomForest(300, feature_types, max_features = int(X_train.shape[1]/3), alpha=alpha, beta=1)\n",
    "        model.fit(X_train.values, y_train.values)\n",
    "        preds = model.predict(X_test.values)\n",
    "        #calculating accuracy\n",
    "        acc = sum(preds == y_test.values)/len(y_test)\n",
    "        #calculating f1 score\n",
    "        f = f1_score(y_test.values, preds, average=None)\n",
    "\n",
    "        datax = [alpha, beta, acc, f]\n",
    "        # write the data\n",
    "        writer.writerow(datax)\n",
    "        myfile.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
