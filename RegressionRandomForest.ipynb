{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from numba import jit, njit, prange\n",
    "from numba.types import bool_, int_, float32\n",
    "from joblib import Parallel, delayed\n",
    "import csv\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bootstrap(X_train, y_train):\n",
    "    #function for generating bootstrap samples (for bagging)\n",
    "    bootstrap_indices = np.random.choice(range(len(X_train)), len(X_train), replace = True)\n",
    "    X_bootstrap = np.take(X_train, bootstrap_indices, axis = 0)\n",
    "    y_bootstrap = np.take(y_train,bootstrap_indices)\n",
    "    return X_bootstrap, y_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regr_parameters(x,y):\n",
    "    # function for estimating sigma^2 for regression  \n",
    "    regr = LinearRegression(n_jobs=-1) \n",
    "    regr.fit(x,y)    \n",
    "    y_pred = regr.predict(x)\n",
    "    sigma_squared = mean_squared_error(y, y_pred)\n",
    "    return sigma_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(x, y, alpha, feature, min_samples_leaf, beta=1, entropy_type = 'Renyi'):\n",
    "    #function for finding the best split by one chosen attribute; 'feature' is an attribute number \n",
    "    n = y.shape[0]   #number of samples in train dataset\n",
    "    thresholds = np.unique(x[:,feature])  #vector of unique values of the feature\n",
    "    info_gain = np.empty(thresholds.shape[0]-1)  #form a vector for infromation gain values\n",
    "    info_gain.fill(-99999)  #and fill it with large negative numbers\n",
    "    \n",
    "    #calculate sigma^2 before splitting \n",
    "    sigma_squared = regr_parameters(x,y)\n",
    "    \n",
    "    j = 0\n",
    "    for split_point in thresholds[0:-1:1]: \n",
    "        #cycle by possible values of the chosen feature for splitting (without the largest value)\n",
    "        #if the feature vector contains all the same values (that is, only one unique value), then splitting cannot be done \n",
    "        #building a split into left and right subtrees for a specific split_point value\n",
    "        feature_vector = x[:, feature]\n",
    "        split = feature_vector <= split_point\n",
    "        x_left = x[split]\n",
    "        y_left = y[split]\n",
    "        x_right = x[np.logical_not(split)]\n",
    "        y_right = y[np.logical_not(split)]\n",
    "           \n",
    "        #check that there are enough observations in the left and right subtrees\n",
    "        if ((y_left.shape[0] >= min_samples_leaf) and (y_right.shape[0] >= min_samples_leaf)):   \n",
    "            #estimate sigma^2 for left and right subtrees\n",
    "            sigma_squared_left = regr_parameters(x_left,y_left)\n",
    "            sigma_squared_right = regr_parameters(x_right,y_right)\n",
    "            \n",
    "            #calculating information gain (if alpha=1, then Shannon entropy is used; if beta=1, then Renyi entropy is used; otherwise Sharma-Mittal entropy is used)\n",
    "            if (entropy_type == 'SharmaMittal') and (alpha != 1) and (beta!=1): \n",
    "                #calculating Sharma-Mittal entropy-based information gain \n",
    "                info_gain_temp = (1/(1-beta))*(1/((np.power(np.sqrt(sigma_squared),beta-1))* (np.power(np.sqrt(2*np.pi), beta-1)) * (np.power(np.sqrt(alpha),(1-beta)/(1-alpha))))-1) -((y_left.shape[0])/(y.shape[0]))* (1/(1-beta))*(1/((np.power(np.sqrt(sigma_squared_left),beta-1))* (np.power(np.sqrt(2*np.pi), beta-1)) * (np.power(np.sqrt(alpha),(1-beta)/(1-alpha))))-1) - ((y_right.shape[0])/(y.shape[0]))*(1/(1-beta))*(1/((np.power(np.sqrt(sigma_squared_right),beta-1))* (np.power(np.sqrt(2*np.pi), beta-1)) * (np.power(np.sqrt(alpha),(1-beta)/(1-alpha))))-1)\n",
    "                info_gain[j] = info_gain_temp \n",
    "            else:         \n",
    "                if alpha == 1:\n",
    "                    #calculating Shannon entropy-based information gain\n",
    "                    info_gain_temp = (1/2)*np.log(2*np.e*np.pi)+np.log(np.sqrt(sigma_squared))-((y_left.shape[0])/(y.shape[0]))*((1/2)*np.log(2*np.e*np.pi)+np.log(np.sqrt(sigma_squared_left)))-((y_right.shape[0])/(y.shape[0]))*((1/2)*np.log(2*np.e*np.pi)+np.log(np.sqrt(sigma_squared_right)))\n",
    "                    info_gain[j] = info_gain_temp \n",
    "                else:\n",
    "                    #calculating Renyi entropy-based information gain\n",
    "                    info_gain_temp = np.log(np.sqrt(2*np.pi))+np.log(np.sqrt(sigma_squared))-(np.log(np.sqrt(alpha)))/(1-alpha)-((y_left.shape[0])/(y.shape[0]))*(np.log(np.sqrt(2*np.pi))+np.log(np.sqrt(sigma_squared_left))-(np.log(np.sqrt(alpha)))/(1-alpha))-((y_right.shape[0])/(y.shape[0]))*(np.log(np.sqrt(2*np.pi))+np.log(np.sqrt(sigma_squared_right))-(np.log(np.sqrt(alpha)))/(1-alpha))\n",
    "                    info_gain[j] = info_gain_temp    \n",
    "        j = j+1  \n",
    "        \n",
    "    \n",
    "    if thresholds.shape[0] > 1:  #if feature has at least two different values, then splitting is possible\n",
    "        best_ind = np.argmax(info_gain) #finding index of the best information gain\n",
    "        best_thr = thresholds[best_ind]\n",
    "        best_info_gain = info_gain[best_ind] #finding the best information gain\n",
    "        if best_info_gain == -99999: #in this case, splitting is impossible\n",
    "            return None, None\n",
    "        else:\n",
    "            return best_thr, best_info_gain\n",
    "    else: #in this case, splitting is impossible\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRenyi_and_SharmaMittal_Regressor:\n",
    "    #class describing a decision tree\n",
    "    def __init__(self, max_features, alpha, beta=1, max_depth=16, min_samples_split=100, min_samples_leaf=10, entropy_type = 'Renyi'):\n",
    "\n",
    "        self._tree = {}\n",
    "        self.max_features = max_features\n",
    "        self._max_depth = max_depth\n",
    "        self._min_samples_split = min_samples_split\n",
    "        self._min_samples_leaf = min_samples_leaf\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.entropy_type = entropy_type\n",
    "        \n",
    "    def _fit_node(self, sub_X, sub_y, node, depth=0):   \n",
    "        if self._max_depth is not None and depth==self._max_depth: #max depth is reached\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            regr = LinearRegression(n_jobs=-1)  \n",
    "            regr.fit(sub_X,sub_y)    \n",
    "            node[\"pred\"] = regr\n",
    "            return    \n",
    "        \n",
    "        if self._min_samples_split is not None and (len(sub_y)<self._min_samples_split): #minimum number of samples is reached\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            regr = LinearRegression(n_jobs=-1) \n",
    "            regr.fit(sub_X,sub_y)    \n",
    "            node[\"pred\"] = regr\n",
    "            return  \n",
    "        \n",
    "        feature_best, threshold_best, info_gain_best = None, None, None\n",
    "        split = None\n",
    "        \n",
    "        #selecting a given number of features in a random manner; these features will be used for finding the best split \n",
    "        for feature in np.random.choice(sub_X.shape[1], self.max_features, replace=False):\n",
    "            threshold, info_gain = find_best_split(sub_X, sub_y, self.alpha, feature, self._min_samples_leaf, self.beta, self.entropy_type)\n",
    "            if info_gain is not None:\n",
    "                if (info_gain_best is None) or (info_gain > info_gain_best):\n",
    "                    feature_best = feature\n",
    "                    info_gain_best = info_gain\n",
    "                    feature_vector = sub_X[:, feature_best]\n",
    "                    split = feature_vector <= threshold\n",
    "                    threshold_best = threshold \n",
    "                \n",
    "        if (info_gain_best is None) or (feature_best is None) or (info_gain_best==-99999):\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            regr = LinearRegression(n_jobs=-1) \n",
    "            regr.fit(sub_X,sub_y)\n",
    "            node[\"pred\"] = regr\n",
    "            return   \n",
    "            \n",
    "        node[\"type\"] = \"nonterminal\"\n",
    "        node[\"feature_split\"] = feature_best\n",
    "        node[\"threshold\"] = threshold_best\n",
    "        node[\"left_child\"], node[\"right_child\"] = {}, {} \n",
    "            \n",
    "        self._fit_node(sub_X[split], sub_y[split], node[\"left_child\"], depth=depth+1)  \n",
    "        self._fit_node(sub_X[np.logical_not(split)], sub_y[np.logical_not(split)], node[\"right_child\"], depth=depth+1)        \n",
    "        \n",
    "    def _predict_node(self, x, node):  \n",
    "        #here x is a single observation from test dataset\n",
    "        tree = node\n",
    "        pred_value = None\n",
    "        while True:\n",
    "            if tree['type'] == 'terminal':\n",
    "                pred_value = tree['pred'].predict(x.reshape(1, -1))\n",
    "                break\n",
    "            else:\n",
    "                feature_split_i = tree['feature_split']\n",
    "                real_split_i = tree['threshold']\n",
    "                if x[feature_split_i] < real_split_i:\n",
    "                    tree = tree['left_child']\n",
    "                else:\n",
    "                    tree = tree['right_child']\n",
    "        return pred_value\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.shape = X.shape[1]\n",
    "        self._fit_node(X, y, self._tree)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predicted = []\n",
    "        for i in range(len(X)):\n",
    "            predicted.append(self._predict_node(X[i], self._tree))\n",
    "        return np.array(predicted).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Renyi_and_SharmaMittal_RandomForestRegressor:\n",
    "    #class describing a random forest\n",
    "    def __init__(self, num_trees, max_features, alpha, beta,max_depth, min_samples_split, min_samples_leaf, entropy_type):\n",
    "        \n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.entropy_type = entropy_type\n",
    "        self.num_trees = num_trees\n",
    "        self.forest = []\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.forest = Parallel(n_jobs=-1)(delayed(self.fit_tree)(X_train, y_train) for _ in range(self.num_trees))\n",
    "\n",
    "    def fit_tree(self, X_train, y_train):\n",
    "        X_bootstrap, y_bootstrap = draw_bootstrap(X_train, y_train)\n",
    "        temp = DecisionTreeRenyi_and_SharmaMittal_Regressor(\n",
    "            self.max_features, self.alpha, self.beta, self.max_depth, \n",
    "            self.min_samples_split, self.min_samples_leaf, self.entropy_type)\n",
    "        temp.fit(X_bootstrap, y_bootstrap)\n",
    "        return temp\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        preds = self.forest[0].predict(X_test)\n",
    "        for i in self.forest[1:]:\n",
    "            preds = np.vstack([preds,i.predict(X_test)]) #Stack arrays in sequence vertically (row wise)\n",
    "        preds = np.swapaxes(preds,0,1)  #row corresponds to predictions from different trees for one test sample x\n",
    "        preds = np.array([np.mean(row) for row in preds]) #calcaulating mean value on all trees for each test sample х\n",
    "        return preds      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7560, 6)\n"
     ]
    }
   ],
   "source": [
    "#example for sulfur dataset with Renyi entropy-based information gain\n",
    "\n",
    "data = loadarff('sulfur.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='y1'), df['y1'], test_size=0.25, random_state=42)\n",
    "X_train_np = X_train.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "beta = 1\n",
    "\n",
    "header = ['alpha', 'beta', 'MSE', 'MAE', 'R2']\n",
    "with open('sulfur_results_Renyi_LineByLine.csv', 'a', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    for alpha in tqdm(np.arange(0.01, 1.0, 0.01)):\n",
    "        model = Renyi_and_SharmaMittal_RandomForestRegressor(500, max_features = int(np.sqrt(X_train_np.shape[1])),\n",
    "                alpha=alpha, beta=1, max_depth = 16, min_samples_split=120, min_samples_leaf=60,  entropy_type = 'Renyi')\n",
    "        model.fit(X_train_np, y_train_np)\n",
    "        preds = model.predict(X_test_np)\n",
    "        MSE = mean_squared_error(y_test_np, preds)\n",
    "        MAE = mean_absolute_error(y_test_np, preds)\n",
    "        R2 = r2_score(y_test_np, preds)\n",
    "        datax = [alpha, beta, MSE, MAE, R2]\n",
    "        # write the data\n",
    "        writer.writerow(datax)\n",
    "        f.flush()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example for sulfur dataset with Sharma-Mittal entropy-based information gain\n",
    "\n",
    "data = loadarff('sulfur.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='y1'), df['y1'], test_size=0.25, random_state=42)\n",
    "X_train_np = X_train.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "header = ['alpha', 'beta', 'MSE', 'MAE', 'R2']\n",
    "with open('sulfur_results_SharmaMittal_LineByLine.csv', 'a', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    for alpha in tqdm(np.arange(0.01, 1.0, 0.01)):\n",
    "        for beta in tqdm(np.arange(0.01, 1.0, 0.01)):\n",
    "            model = Renyi_and_SharmaMittal_RandomForestRegressor(500, max_features = int(np.sqrt(X_train_np.shape[1])),\n",
    "                alpha=alpha, beta=beta, max_depth = 16, min_samples_split=120, min_samples_leaf=60,  entropy_type = 'SharmaMittal')\n",
    "            model.fit(X_train_np, y_train_np)\n",
    "            preds = model.predict(X_test_np)\n",
    "            MSE = mean_squared_error(y_test_np, preds)\n",
    "            MAE = mean_absolute_error(y_test_np, preds)\n",
    "            R2 = r2_score(y_test_np, preds)\n",
    "            datax = [alpha, beta, MSE, MAE, R2]\n",
    "            # write the data\n",
    "            writer.writerow(datax)\n",
    "            f.flush()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "8401446d6ec83e583273fb275839998277de92b3d387076cd1f5e9ac53d1ab81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
